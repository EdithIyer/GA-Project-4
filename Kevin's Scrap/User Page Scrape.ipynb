{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import plotly\n",
    "import html\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @thejakartaglobe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I used Firefox; you can use whichever browser you like.\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# Tell Selenium to get the URL you're interested in.\n",
    "browser.get(\"https://twitter.com/thejakartaglobe\")\n",
    "\n",
    "# Selenium script to scroll to the bottom, wait 3 seconds for the next batch of data to load, then continue scrolling.  It will continue to do this until the page stops loading new data.\n",
    "lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "while(match==False):\n",
    "        lastCount = lenOfPage\n",
    "        time.sleep(1)\n",
    "        lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        if lastCount==lenOfPage:\n",
    "            match=True\n",
    "\n",
    "# Now that the page is fully scrolled, grab the source code.\n",
    "source_data = browser.page_source\n",
    "\n",
    "# Throw your source into BeautifulSoup and start parsing!\n",
    "bs_data = bs(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tweets = len(list(bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})))\n",
    "num_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "260\n",
      "265\n",
      "270\n",
      "275\n",
      "280\n",
      "285\n",
      "290\n",
      "295\n",
      "300\n",
      "305\n",
      "310\n",
      "315\n",
      "320\n",
      "325\n",
      "330\n",
      "335\n",
      "340\n",
      "345\n",
      "350\n",
      "355\n",
      "360\n",
      "365\n",
      "370\n",
      "375\n",
      "380\n",
      "385\n",
      "390\n",
      "395\n",
      "400\n",
      "405\n",
      "410\n",
      "415\n",
      "420\n",
      "425\n",
      "430\n",
      "435\n",
      "440\n",
      "445\n",
      "450\n",
      "455\n",
      "460\n",
      "465\n",
      "470\n",
      "475\n",
      "480\n",
      "485\n",
      "490\n",
      "495\n",
      "500\n",
      "505\n",
      "510\n",
      "515\n",
      "520\n",
      "525\n",
      "530\n",
      "535\n",
      "540\n",
      "545\n",
      "550\n",
      "555\n",
      "560\n",
      "565\n",
      "570\n",
      "575\n",
      "580\n",
      "585\n",
      "590\n",
      "595\n",
      "600\n",
      "605\n",
      "610\n",
      "615\n",
      "620\n",
      "625\n",
      "630\n",
      "635\n",
      "640\n",
      "645\n",
      "650\n",
      "655\n",
      "660\n",
      "665\n",
      "670\n",
      "675\n",
      "680\n",
      "685\n",
      "690\n",
      "695\n",
      "700\n",
      "705\n",
      "710\n",
      "715\n",
      "720\n",
      "725\n",
      "730\n",
      "735\n",
      "740\n",
      "745\n",
      "750\n",
      "755\n",
      "760\n",
      "765\n",
      "770\n",
      "775\n",
      "780\n",
      "785\n",
      "790\n",
      "795\n",
      "800\n",
      "805\n",
      "810\n",
      "815\n",
      "820\n",
      "825\n"
     ]
    }
   ],
   "source": [
    "master_list = []\n",
    "\n",
    "for i in range(num_tweets):\n",
    "    tweets = bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})\n",
    "    \n",
    "    temp_dict = {}\n",
    "\n",
    "    temp_dict['handle']     = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('href=\"/')[1].split('/status')[0]\n",
    "    \n",
    "    try:\n",
    "        temp_dict['tweet']      = str(tweets[i].get_text()).replace(\"\\n\", \" \")\n",
    "        #temp_dict['tweet']      = str(list(bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'}))[i]).split('lang=\"en\">')[1].split('<a class')[0].rstrip()\n",
    "    except IndexError:\n",
    "        temp_dict['tweet']      = \"\"\n",
    "    \n",
    "    temp_dict['day']        = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[0]\n",
    "    temp_dict['month']      = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[1]\n",
    "    temp_dict['year']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[2]\n",
    "    temp_dict['time']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split(' - ')[0]\n",
    "    \n",
    "    datetime_object = datetime.strptime(temp_dict['month'] + ' ' + temp_dict['day'] + ' ' + temp_dict['year'] + ' ' + temp_dict['time'].replace(\" \", \"\"), '%b %d %Y %I:%M%p')\n",
    "    \n",
    "    temp_dict['datetime']   = datetime_object\n",
    "    temp_dict['if_tsunami'] = 'tsunami' in temp_dict['tweet'] or 'Tsunami' in temp_dict['tweet'] or '#tsunami' in temp_dict['tweet'] or '#Tsunami' in temp_dict['tweet']\n",
    "    \n",
    "    master_list.append(temp_dict)\n",
    "    if i % 5 == 0:\n",
    "        print(i)\n",
    "    else:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_jakarta_globe = pd.DataFrame(master_list)\n",
    "the_jakarta_globe = the_jakarta_globe[['handle', 'tweet', 'if_tsunami', 'datetime', 'day', 'month', 'year', 'time']]\n",
    "the_jakarta_globe.to_csv('the_jakarta_globe.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Sutopo_PN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I used Firefox; you can use whichever browser you like.\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# Tell Selenium to get the URL you're interested in.\n",
    "browser.get(\"https://twitter.com/Sutopo_PN\")\n",
    "\n",
    "# Selenium script to scroll to the bottom, wait 3 seconds for the next batch of data to load, then continue scrolling.  It will continue to do this until the page stops loading new data.\n",
    "lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "while(match==False):\n",
    "        lastCount = lenOfPage\n",
    "        time.sleep(1)\n",
    "        lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        if lastCount==lenOfPage:\n",
    "            match=True\n",
    "\n",
    "# Now that the page is fully scrolled, grab the source code.\n",
    "source_data = browser.page_source\n",
    "\n",
    "# Throw your source into BeautifulSoup and start parsing!\n",
    "bs_data = bs(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tweets = len(list(bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})))\n",
    "num_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "260\n",
      "265\n",
      "270\n",
      "275\n",
      "280\n",
      "285\n",
      "290\n",
      "295\n",
      "300\n",
      "305\n",
      "310\n",
      "315\n",
      "320\n",
      "325\n",
      "330\n",
      "335\n",
      "340\n",
      "345\n",
      "350\n",
      "355\n",
      "360\n",
      "365\n",
      "370\n",
      "375\n",
      "380\n",
      "385\n",
      "390\n",
      "395\n",
      "400\n",
      "405\n",
      "410\n",
      "415\n",
      "420\n",
      "425\n",
      "430\n",
      "435\n",
      "440\n",
      "445\n",
      "450\n",
      "455\n",
      "460\n",
      "465\n",
      "470\n",
      "475\n",
      "480\n",
      "485\n",
      "490\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "master_list = []\n",
    "\n",
    "for i in range(num_tweets):\n",
    "    tweets = bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})\n",
    "    \n",
    "    temp_dict = {}\n",
    "\n",
    "    temp_dict['handle']     = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('href=\"/')[1].split('/status')[0]\n",
    "    \n",
    "    try:\n",
    "        temp_dict['tweet']      = str(tweets[i].get_text()).replace(\"\\n\", \" \")\n",
    "        #temp_dict['tweet']      = str(list(bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'}))[i]).split('lang=\"en\">')[1].split('<a class')[0].rstrip()\n",
    "    except IndexError:\n",
    "        temp_dict['tweet']      = \"\"\n",
    "    \n",
    "    temp_dict['day']        = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[0]\n",
    "    temp_dict['month']      = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[1]\n",
    "    temp_dict['year']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[2]\n",
    "    temp_dict['time']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split(' - ')[0]\n",
    "    \n",
    "    datetime_object = datetime.strptime(temp_dict['month'] + ' ' + temp_dict['day'] + ' ' + temp_dict['year'] + ' ' + temp_dict['time'].replace(\" \", \"\"), '%b %d %Y %I:%M%p')\n",
    "    \n",
    "    temp_dict['datetime']   = datetime_object\n",
    "    temp_dict['if_tsunami'] = 'tsunami' in temp_dict['tweet'] or 'Tsunami' in temp_dict['tweet'] or '#tsunami' in temp_dict['tweet'] or '#Tsunami' in temp_dict['tweet']\n",
    "    \n",
    "    master_list.append(temp_dict)\n",
    "    if i % 5 == 0:\n",
    "        print(i)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sutopo_pn = pd.DataFrame(master_list)\n",
    "sutopo_pn = sutopo_pn[['handle', 'tweet', 'if_tsunami', 'datetime', 'day', 'month', 'year', 'time']]\n",
    "sutopo_pn.to_csv('sutopo_pn.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @BNPB_Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I used Firefox; you can use whichever browser you like.\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# Tell Selenium to get the URL you're interested in.\n",
    "browser.get(\"https://twitter.com/BNPB_Indonesia\")\n",
    "\n",
    "# Selenium script to scroll to the bottom, wait 3 seconds for the next batch of data to load, then continue scrolling.  It will continue to do this until the page stops loading new data.\n",
    "lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "while(match==False):\n",
    "        lastCount = lenOfPage\n",
    "        time.sleep(1)\n",
    "        lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        if lastCount==lenOfPage:\n",
    "            match=True\n",
    "\n",
    "# Now that the page is fully scrolled, grab the source code.\n",
    "source_data = browser.page_source\n",
    "\n",
    "# Throw your source into BeautifulSoup and start parsing!\n",
    "bs_data = bs(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tweets = len(list(bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})))\n",
    "num_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "260\n",
      "265\n",
      "270\n",
      "275\n",
      "280\n",
      "285\n",
      "290\n",
      "295\n",
      "300\n",
      "305\n",
      "310\n",
      "315\n",
      "320\n",
      "325\n",
      "330\n",
      "335\n",
      "340\n",
      "345\n",
      "350\n",
      "355\n",
      "360\n",
      "365\n",
      "370\n",
      "375\n",
      "380\n",
      "385\n",
      "390\n",
      "395\n",
      "400\n",
      "405\n",
      "410\n",
      "415\n",
      "420\n",
      "425\n",
      "430\n",
      "435\n",
      "440\n",
      "445\n",
      "450\n",
      "455\n",
      "460\n",
      "465\n",
      "470\n",
      "475\n",
      "480\n",
      "485\n",
      "490\n",
      "495\n",
      "500\n",
      "505\n",
      "510\n",
      "515\n",
      "520\n",
      "525\n",
      "530\n",
      "535\n",
      "540\n",
      "545\n",
      "550\n",
      "555\n",
      "560\n",
      "565\n",
      "570\n",
      "575\n",
      "580\n",
      "585\n",
      "590\n",
      "595\n",
      "600\n",
      "605\n",
      "610\n",
      "615\n",
      "620\n",
      "625\n",
      "630\n",
      "635\n",
      "640\n",
      "645\n",
      "650\n",
      "655\n",
      "660\n",
      "665\n",
      "670\n",
      "675\n",
      "680\n",
      "685\n",
      "690\n",
      "695\n",
      "700\n",
      "705\n",
      "710\n",
      "715\n",
      "720\n",
      "725\n",
      "730\n",
      "735\n",
      "740\n",
      "745\n",
      "750\n",
      "755\n",
      "760\n",
      "765\n",
      "770\n",
      "775\n",
      "780\n",
      "785\n",
      "790\n",
      "795\n",
      "800\n",
      "805\n",
      "810\n",
      "815\n",
      "820\n"
     ]
    }
   ],
   "source": [
    "master_list = []\n",
    "\n",
    "for i in range(num_tweets):\n",
    "    tweets = bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})\n",
    "    \n",
    "    temp_dict = {}\n",
    "\n",
    "    temp_dict['handle']     = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('href=\"/')[1].split('/status')[0]\n",
    "    \n",
    "    try:\n",
    "        temp_dict['tweet']      = str(tweets[i].get_text()).replace(\"\\n\", \" \")\n",
    "        #temp_dict['tweet']      = str(list(bs_data.find_all('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'}))[i]).split('lang=\"en\">')[1].split('<a class')[0].rstrip()\n",
    "    except IndexError:\n",
    "        temp_dict['tweet']      = \"\"\n",
    "    \n",
    "    temp_dict['day']        = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[0]\n",
    "    temp_dict['month']      = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[1]\n",
    "    temp_dict['year']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[2]\n",
    "    temp_dict['time']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split(' - ')[0]\n",
    "    \n",
    "    datetime_object = datetime.strptime(temp_dict['month'] + ' ' + temp_dict['day'] + ' ' + temp_dict['year'] + ' ' + temp_dict['time'].replace(\" \", \"\"), '%b %d %Y %I:%M%p')\n",
    "    \n",
    "    temp_dict['datetime']   = datetime_object\n",
    "    temp_dict['if_tsunami'] = 'tsunami' in temp_dict['tweet'] or 'Tsunami' in temp_dict['tweet'] or '#tsunami' in temp_dict['tweet'] or '#Tsunami' in temp_dict['tweet']\n",
    "    \n",
    "    master_list.append(temp_dict)\n",
    "    if i % 5 == 0:\n",
    "        print(i)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bnpb_indonesia = pd.DataFrame(master_list)\n",
    "bnpb_indonesia = bnpb_indonesia[['handle', 'tweet', 'if_tsunami', 'datetime', 'day', 'month', 'year', 'time']]\n",
    "bnpb_indonesia.to_csv('bnpb_indonesia.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #tsunami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# I used Firefox; you can use whichever browser you like.\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# Tell Selenium to get the URL you're interested in.\n",
    "browser.get(\"https://twitter.com/hashtag/tsunami?lang=en\")\n",
    "\n",
    "# Selenium script to scroll to the bottom, wait 3 seconds for the next batch of data to load, then continue scrolling.  It will continue to do this until the page stops loading new data.\n",
    "lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "while(match==False):\n",
    "        lastCount = lenOfPage\n",
    "        time.sleep(1)\n",
    "        lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        if lastCount==lenOfPage:\n",
    "            match=True\n",
    "\n",
    "# Now that the page is fully scrolled, grab the source code.\n",
    "source_data = browser.page_source\n",
    "\n",
    "# Throw your source into BeautifulSoup and start parsing!\n",
    "bs_data = bs(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tweets = len(list(bs_data.find_all('p', {'class': 'TweetTextSize js-tweet-text tweet-text'})))\n",
    "num_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "260\n",
      "265\n",
      "270\n",
      "275\n",
      "280\n",
      "285\n",
      "290\n",
      "295\n",
      "300\n",
      "305\n",
      "310\n",
      "315\n",
      "320\n",
      "325\n",
      "330\n",
      "335\n",
      "340\n",
      "345\n",
      "350\n",
      "355\n",
      "360\n",
      "365\n",
      "370\n",
      "375\n",
      "380\n",
      "385\n",
      "390\n",
      "395\n",
      "400\n",
      "405\n",
      "410\n",
      "415\n",
      "420\n",
      "425\n",
      "430\n",
      "435\n",
      "440\n",
      "445\n",
      "450\n",
      "455\n",
      "460\n",
      "465\n",
      "470\n",
      "475\n",
      "480\n",
      "485\n",
      "490\n",
      "495\n",
      "500\n",
      "505\n",
      "510\n",
      "515\n",
      "520\n",
      "525\n",
      "530\n",
      "535\n",
      "540\n",
      "545\n",
      "550\n",
      "555\n",
      "560\n",
      "565\n",
      "570\n",
      "575\n",
      "580\n",
      "585\n",
      "590\n",
      "595\n",
      "600\n",
      "605\n",
      "610\n",
      "615\n",
      "620\n",
      "625\n",
      "630\n",
      "635\n",
      "640\n",
      "645\n",
      "650\n",
      "655\n",
      "660\n",
      "665\n",
      "670\n",
      "675\n",
      "680\n",
      "685\n",
      "690\n",
      "695\n",
      "700\n",
      "705\n",
      "710\n",
      "715\n",
      "720\n",
      "725\n",
      "730\n",
      "735\n",
      "740\n",
      "745\n",
      "750\n",
      "755\n",
      "760\n",
      "765\n",
      "770\n",
      "775\n",
      "780\n",
      "785\n",
      "790\n",
      "795\n",
      "800\n",
      "805\n",
      "810\n",
      "815\n",
      "820\n",
      "825\n",
      "830\n",
      "835\n",
      "840\n",
      "845\n",
      "850\n",
      "855\n",
      "860\n",
      "865\n",
      "870\n",
      "875\n",
      "880\n",
      "885\n",
      "890\n",
      "895\n",
      "900\n",
      "905\n",
      "910\n",
      "915\n",
      "920\n",
      "925\n",
      "930\n",
      "935\n",
      "940\n",
      "945\n",
      "950\n",
      "955\n",
      "960\n",
      "965\n",
      "970\n",
      "975\n",
      "980\n",
      "985\n",
      "990\n",
      "995\n",
      "1000\n",
      "1005\n",
      "1010\n",
      "1015\n",
      "1020\n",
      "1025\n",
      "1030\n",
      "1035\n",
      "1040\n",
      "1045\n",
      "1050\n",
      "1055\n",
      "1060\n",
      "1065\n",
      "1070\n",
      "1075\n",
      "1080\n",
      "1085\n",
      "1090\n",
      "1095\n",
      "1100\n",
      "1105\n",
      "1110\n",
      "1115\n",
      "1120\n",
      "1125\n",
      "1130\n",
      "1135\n",
      "1140\n",
      "1145\n",
      "1150\n",
      "1155\n",
      "1160\n",
      "1165\n",
      "1170\n",
      "1175\n",
      "1180\n",
      "1185\n",
      "1190\n",
      "1195\n",
      "1200\n",
      "1205\n",
      "1210\n",
      "1215\n",
      "1220\n",
      "1225\n",
      "1230\n",
      "1235\n",
      "1240\n",
      "1245\n",
      "1250\n",
      "1255\n",
      "1260\n",
      "1265\n",
      "1270\n",
      "1275\n",
      "1280\n",
      "1285\n",
      "1290\n",
      "1295\n",
      "1300\n",
      "1305\n",
      "1310\n",
      "1315\n",
      "1320\n",
      "1325\n",
      "1330\n",
      "1335\n",
      "1340\n",
      "1345\n",
      "1350\n",
      "1355\n",
      "1360\n",
      "1365\n",
      "1370\n",
      "1375\n",
      "1380\n",
      "1385\n",
      "1390\n",
      "1395\n",
      "1400\n",
      "1405\n",
      "1410\n",
      "1415\n"
     ]
    }
   ],
   "source": [
    "master_list = []\n",
    "#loop_range = 10\n",
    "\n",
    "\n",
    "for i in range(num_tweets):\n",
    "    tweets = bs_data.find_all('p', {'class': 'TweetTextSize js-tweet-text tweet-text'})\n",
    "\n",
    "    temp_dict = {}\n",
    "\n",
    "    temp_dict['handle']     = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('href=\"/')[1].split('/status')[0]\n",
    "    \n",
    "    try:\n",
    "        temp_dict['tweet']      = str(tweets[i].get_text()).replace(\"\\n\", \" \")\n",
    "        #temp_dict['tweet']      = str(list(bs_data.find_all('p', {bs_data.find_all('p', {'class': 'TweetTextSize js-tweet-text tweet-text'})[i].split('lang=\"en\">')[1].split('<a class')[0].rstrip()\n",
    "    except IndexError:\n",
    "        temp_dict['tweet']      = \"\"\n",
    "    \n",
    "    temp_dict['day']        = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[0]\n",
    "    temp_dict['month']      = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[1]\n",
    "    temp_dict['year']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split('-')[1].lstrip().split(' ')[2]\n",
    "    temp_dict['time']       = str(list(bs_data.find_all('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}))[i]).split('title=\"')[1].split('\"><span ')[0].split(' - ')[0]\n",
    "    \n",
    "    datetime_object = datetime.strptime(temp_dict['month'] + ' ' + temp_dict['day'] + ' ' + temp_dict['year'] + ' ' + temp_dict['time'].replace(\" \", \"\"), '%b %d %Y %I:%M%p')\n",
    "    \n",
    "    temp_dict['datetime']   = datetime_object\n",
    "    temp_dict['if_tsunami'] = 'tsunami' in temp_dict['tweet'] or 'Tsunami' in temp_dict['tweet'] or '#tsunami' in temp_dict['tweet'] or '#Tsunami' in temp_dict['tweet'] or '#TSUNAMI' in temp_dict['tweet']\n",
    "    \n",
    "    master_list.append(temp_dict)\n",
    "    if i % 5 == 0:\n",
    "        print(i)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash_tsunami = pd.DataFrame(master_list)\n",
    "#hash_tsunami = hash_tsunami[['handle', 'tweet', 'if_tsunami', 'datetime', 'day', 'month', 'year', 'time']]\n",
    "hash_tsunami.to_csv('hash_tsunami.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_tsunami['if_alert_worthy'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_tsunami['if_tsunami'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>if_tsunami</th>\n",
       "      <th>datetime</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>time</th>\n",
       "      <th>if_alert_worthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UiB_Rotevatn</td>\n",
       "      <td>. @KateClarkEqGeo @gnsscience bringing New Zea...</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-15 08:07:00</td>\n",
       "      <td>15</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2019</td>\n",
       "      <td>8:07 AM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>napakcha</td>\n",
       "      <td>How to survive a #tsunami! https://www.faceboo...</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-15 04:59:00</td>\n",
       "      <td>15</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2019</td>\n",
       "      <td>4:59 AM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geotecniaonline</td>\n",
       "      <td>raspishake: #Indonesia is a nation of islands ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-15 06:46:00</td>\n",
       "      <td>15</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2019</td>\n",
       "      <td>6:46 AM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            handle                                              tweet  \\\n",
       "0     UiB_Rotevatn  . @KateClarkEqGeo @gnsscience bringing New Zea...   \n",
       "1         napakcha  How to survive a #tsunami! https://www.faceboo...   \n",
       "2  geotecniaonline  raspishake: #Indonesia is a nation of islands ...   \n",
       "\n",
       "   if_tsunami            datetime day month  year     time  if_alert_worthy  \n",
       "0        True 2019-01-15 08:07:00  15   Jan  2019  8:07 AM                0  \n",
       "1        True 2019-01-15 04:59:00  15   Jan  2019  4:59 AM                0  \n",
       "2        True 2019-01-15 06:46:00  15   Jan  2019  6:46 AM                0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_tsunami.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
